### (OL-MCP) Local AI Stack Docker Compose

services:
  # Primary Vector Database - Qdrant with built-in embedding support
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: vector-database
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      # Enable built-in transformers
      - QDRANT__SERVICE__ENABLE_CORS=true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Alternative: All-in-one Qdrant with transformers (comment out above qdrant service to use this)
  # qdrant-transformers:
  #   image: qdrant/qdrant-transformers:latest
  #   container_name: qdrant-with-transformers
  #   ports:
  #     - "6333:6333"
  #     - "8080:8080"  # Transformers API
  #   volumes:
  #     - qdrant_data:/qdrant/storage
  #     - transformers_cache:/root/.cache/huggingface
  #   environment:
  #     - QDRANT_HOST=0.0.0.0
  #     - TRANSFORMERS_CACHE=/root/.cache/huggingface
  #     - DEFAULT_MODEL=sentence-transformers/all-MiniLM-L6-v2
  #   restart: unless-stopped

  # Alternative: Weaviate with built-in vectorization
  # weaviate:
  #   image: semitechnologies/weaviate:1.23.7
  #   container_name: weaviate-vectordb
  #   ports:
  #     - "6333:8080"
  #   environment:
  #     - QUERY_DEFAULTS_LIMIT=25
  #     - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
  #     - PERSISTENCE_DATA_PATH=/var/lib/weaviate
  #     - DEFAULT_VECTORIZER_MODULE=text2vec-transformers
  #     - ENABLE_MODULES=text2vec-transformers
  #     - TRANSFORMERS_INFERENCE_API=http://t2v-transformers:8080
  #   volumes:
  #     - weaviate_data:/var/lib/weaviate
  #   restart: unless-stopped

  # # Weaviate transformer module (if using Weaviate)
  # t2v-transformers:
  #   image: semitechnologies/transformers-inference:sentence-transformers-all-MiniLM-L6-v2
  #   container_name: weaviate-transformers
  #   environment:
  #     - ENABLE_CUDA=0
  #   restart: unless-stopped

  # Simplified Embedding Service (now just a proxy to vector DB)
  embedding-proxy:
    build: 
      context: ./embedding-proxy
      dockerfile: Dockerfile
    container_name: embedding-proxy
    ports:
      - "8080:8080"
    environment:
      - VECTOR_DB_URL=http://qdrant:6333
      - VECTOR_DB_TYPE=qdrant
      - EMBEDDING_ENDPOINT=http://qdrant:6333/collections/documents/points/search
    depends_on:
      qdrant:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama LLM Server (unchanged)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # Enhanced MCP Filesystem (simplified for direct vector DB integration)
  mcp-filesystem:
    build: 
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: mcp-filesystem
    ports:
      - "3000:3000"
    volumes:
      - ./shared-data/context-files:/workspace
    environment:
      - NODE_ENV=production
      - VECTOR_DB_URL=http://qdrant:6333
      - VECTOR_DB_TYPE=qdrant
      - COLLECTION_NAME=documents
      - EMBEDDING_PROXY_URL=http://embedding-proxy:8080
    depends_on:
      qdrant:
        condition: service_healthy
      embedding-proxy:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/status"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Flask Application (simplified embedding service calls)
  flask-app:
    build: 
      context: ./flask-app
      dockerfile: Dockerfile
    container_name: flask-ai-interface
    ports:
      - "5000:5000"
    volumes:
      - ./shared-data:/app/shared-data
    environment:
      - FLASK_ENV=production
      - OLLAMA_HOST=http://ollama:11434
      - MCP_SERVER_URL=http://mcp-filesystem:3000
      - VECTOR_DB_URL=http://qdrant:6333
      - EMBEDDING_PROXY_URL=http://embedding-proxy:8080
    depends_on:
      - ollama
      - mcp-filesystem
      - qdrant
    restart: unless-stopped

  # Redis for caching (unchanged)
  redis:
    image: redis:7-alpine
    container_name: redis-cache
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  ollama_data:
    driver: local
  qdrant_data:
    driver: local
  transformers_cache:
    driver: local
  weaviate_data:
    driver: local
  redis_data:
    driver: local

networks:
  default:
    name: enhanced-ai-stack-network
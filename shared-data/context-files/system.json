{
  "system_overview": {
    "name": "Local AI Stack (OL-MCP)",
    "version": "2.0.0",
    "description": "Self-hosted AI chat interface with vectorized RAG pipeline and enhanced reasoning capabilities",
    "mission": "Democratize access to advanced AI capabilities while maintaining complete data privacy and control through local deployment, enhanced with cutting-edge vectorization and multi-step reasoning systems",
    "license": "MIT",
    "github_repository": "https://github.com/trevino293/local_ai_stack"
  },

  "key_features": {
    "vectorization": {
      "semantic_search": "Advanced semantic search using sentence transformers with configurable similarity thresholds",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "vector_dimensions": 384,
      "similarity_threshold": 0.3,
      "chunk_processing": "512 tokens per chunk with 50 token overlap"
    },
    "reasoning": {
      "multi_step_processing": "Six-stage deliberation system",
      "stages": [
        "Problem Decomposition",
        "Evidence Gathering", 
        "Pattern Identification",
        "Hypothesis Formation",
        "Verification",
        "Synthesis"
      ],
      "confidence_scoring": "AI-generated confidence levels (1-10) with detailed breakdowns",
      "transparency": "Complete reasoning process visibility with collapsible sections"
    },
    "rag_pipeline": {
      "type": "Vectorized RAG (Retrieval-Augmented Generation)",
      "processing_modes": {
        "fast": "3 semantic chunks, ~2-3s response time",
        "detailed": "5+ semantic chunks, ~5-7s response time with analysis"
      },
      "citation_tracking": "Automatic source references with relevance scoring",
      "context_continuity": "Maintains conversation history across sessions"
    },
    "interface": {
      "design": "Modern dark blue theme with responsive layout",
      "features": [
        "Interactive reasoning sections",
        "Real-time chat history",
        "Advanced parameter configuration",
        "Context file management",
        "System status monitoring"
      ]
    }
  },

  "system_architecture": {
    "deployment": "Docker Compose containerized services",
    "components": {
      "flask_app": {
        "port": 5000,
        "role": "Main web interface and vectorized RAG pipeline",
        "technology": "Python Flask",
        "responsibilities": [
          "Enhanced RAG pipeline with semantic search",
          "Multi-step reasoning with confidence scoring",
          "Session management and conversation state",
          "Parameter persistence and preset management"
        ]
      },
      "mcp_filesystem": {
        "port": 3000,
        "role": "File management with vectorization",
        "technology": "Node.js Express",
        "responsibilities": [
          "File upload with automatic vectorization",
          "Semantic search API with similarity scoring",
          "Vector index management",
          "System file protection"
        ]
      },
      "embedding_service": {
        "port": 8080,
        "role": "Text vectorization service",
        "technology": "Python with sentence-transformers",
        "model": "all-MiniLM-L6-v2 (22MB, 384 dimensions)",
        "responsibilities": [
          "High-quality text embedding generation",
          "Batch processing for large documents",
          "Health monitoring and performance tracking"
        ]
      },
      "ollama_server": {
        "port": 11434,
        "role": "Local LLM hosting and inference",
        "technology": "Ollama",
        "features": [
          "Multiple model support",
          "GPU acceleration",
          "Parameter-based generation",
          "Model management and switching"
        ]
      }
    },
    "data_flow": "User Query → Vectorization → Semantic Search → Multi-Stage Reasoning → Enhanced Response",
    "storage": {
      "shared_data": "/workspace and /context-files volumes",
      "vector_index": "In-memory with persistent storage",
      "conversations": "Session-based with history tracking"
    }
  },

  "performance_metrics": {
    "speed_improvements": {
      "simple_qa": "60% faster (8s → 3s)",
      "document_analysis": "75% faster (25s → 6s)", 
      "multi_document": "70% faster (45s → 12s)",
      "gpu_acceleration": "50-60% additional speedup"
    },
    "accuracy_improvements": {
      "semantic_relevance": "85% → 92%",
      "citation_precision": "78% → 89%",
      "answer_quality": "82% → 91%",
      "context_utilization": "65% → 88%"
    },
    "resource_usage": {
      "base_memory": "2GB + 1GB per 1000 documents",
      "vector_storage": "~50KB per document",
      "cpu_embedding": "~30% during indexing, ~5% per query",
      "gpu_optional": "2-3x faster inference when available"
    }
  },

  "configuration": {
    "model_parameters": {
      "presets": {
        "creative": {
          "temperature": 1.2,
          "top_p": 0.95,
          "top_k": 50,
          "repeat_penalty": 1.0,
          "use_case": "Creative writing, brainstorming"
        },
        "balanced": {
          "temperature": 0.7,
          "top_p": 0.9,
          "top_k": 40,
          "repeat_penalty": 1.1,
          "use_case": "General conversation, analysis"
        },
        "precise": {
          "temperature": 0.2,
          "top_p": 0.7,
          "top_k": 20,
          "repeat_penalty": 1.2,
          "use_case": "Factual queries, code generation"
        }
      },
      "custom_configurations": "User can save and apply named parameter sets"
    },
    "vectorization_settings": {
      "similarity_threshold": 0.3,
      "top_k_chunks": "3-5 based on processing mode",
      "chunk_size": 512,
      "chunk_overlap": 50,
      "fast_mode_chunks": 3,
      "detailed_mode_chunks": 5
    }
  },

  "api_endpoints": {
    "chat": {
      "POST /api/chat": "Enhanced chat with vectorized RAG",
      "GET /api/chat/history": "Retrieve conversation history",
      "DELETE /api/chat/history": "Clear chat history"
    },
    "vectorization": {
      "POST /search": "Semantic search in context files",
      "POST /embed": "Generate text embeddings",
      "GET /search/stats": "Search performance metrics"
    },
    "file_management": {
      "GET /api/files": "List context files",
      "POST /api/files": "Upload file with auto-vectorization", 
      "DELETE /api/files/{filename}": "Delete file and vectors"
    },
    "configuration": {
      "GET /api/models": "List available Ollama models",
      "POST /api/model-params": "Save model parameters",
      "GET /api/saved-configs": "List saved configurations"
    },
    "monitoring": {
      "GET /api/system/health": "System health check",
      "GET /api/mcp/status": "MCP server status",
      "GET /health": "Embedding service health"
    }
  },

  "use_cases": {
    "research_development": "Document analysis with semantic search, citation tracking, confidence scoring",
    "education_learning": "Interactive learning with reasoning transparency and problem solving",
    "content_creation": "Creative writing with adjustable parameters and context-aware suggestions",
    "code_development": "Programming assistance with context-aware responses and reasoning explanations",
    "enterprise": "Privacy-focused AI deployment with professional interface and conversation management"
  },

  "security_privacy": {
    "privacy_features": [
      "Complete local processing - no external API calls",
      "No data transmission outside local infrastructure", 
      "Offline operation capability",
      "System file protection from deletion"
    ],
    "deployment_security": [
      "Containerized isolation",
      "Volume-based file access control",
      "Environment variable configuration",
      "Optional authentication layer for production"
    ]
  },

  "installation_requirements": {
    "hardware": {
      "minimum_ram": "8GB recommended",
      "gpu": "NVIDIA GPU optional for acceleration",
      "storage": "Varies based on models and context files"
    },
    "software": {
      "docker": "Docker & Docker Compose (latest versions)",
      "os_support": "Linux, macOS, Windows with Docker Desktop"
    },
    "quick_start": [
      "git clone https://github.com/trevino293/local_ai_stack.git",
      "cd local_ai_stack",
      "mkdir -p shared-data/context-files", 
      "docker-compose up -d",
      "docker exec ollama-server ollama pull llama3.2",
      "Access http://localhost:5000"
    ]
  },

  "supported_models": {
    "tested_models": [
      "llama3.2 - General purpose conversation",
      "mistral - Balanced performance and quality",
      "codellama - Code generation and analysis", 
      "phi3 - Lightweight model option",
      "llava - Vision + text capabilities"
    ],
    "model_switching": "Dynamic model selection through web interface",
    "parameter_persistence": "Model-specific parameter storage and recall"
  },

  "development_info": {
    "technology_stack": {
      "backend": "Python Flask with enhanced RAG pipeline",
      "frontend": "Modern JavaScript with responsive CSS",
      "vectorization": "sentence-transformers with Flask API",
      "file_management": "Node.js Express server",
      "ai_inference": "Ollama local LLM hosting",
      "containerization": "Docker Compose orchestration"
    },
    "contribution_areas": [
      "Vectorization improvements (embedding models, search algorithms)",
      "Reasoning enhancements (additional deliberation stages)",
      "UI/UX improvements (mobile responsiveness, accessibility)",
      "Performance optimizations (caching, async processing)",
      "Security features (authentication, authorization)"
    ]
  },

  "system_status": {
    "current_version": "2.0.0 with vectorization",
    "stability": "Production ready with Docker deployment",
    "active_development": true,
    "community": "Open source with MIT license",
    "documentation": "Comprehensive docs at /docs endpoint"
  }
}